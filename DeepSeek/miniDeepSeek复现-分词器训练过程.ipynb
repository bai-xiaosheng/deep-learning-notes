{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 理论基础\n",
    "## 1. 分词器概念\n",
    "分词器（Tokenizer）是指将自然语言文本转换为模型可理解的向量。包含两个任务，第一个是将自然语言文本进行分词，转换为最小基本单元（单词、子词甚至是单个字符），第二个任务就是将自然语言文本映射为一系列的索引。\n",
    "tokens：可以是单词、子词、甚至是单个字符\n",
    "vocabulary：词表，token到整数索引的映射，通常按照首字母或者出现频率排序\n",
    "\n",
    "## 2. 分词器有什么作用\n",
    "1. 将自然语言文本转换为模型可以输入的序列\n",
    "2. 构建词汇表，用有限个子词覆盖大部分文本\n",
    "3. 处理不在词汇表里面的词（out of vacabulary,OOV问题）：将未遇到的词拆分为词汇表里面的子词。\n",
    "4. 保持文本的语义一致性\n",
    "\n",
    "## 3. 为什么要训练分词器\n",
    "1. 为了得到更小的词汇表，这样可以降低嵌入矩阵的参数量，例如词汇表大小为1000，嵌入维度为512，那么嵌入矩阵的大小为（1000，512）\n",
    "2. 提高模型的泛化性：不认识的词可以拆分成多个子词\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备\n",
    "一问一答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Generator\n",
    "\n",
    "def read_text_from_jsonl(file_path: str) -> Generator[str, None, None]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # 字典的形式加载\n",
    "            data = json.loads(line)\n",
    "            # yield关键字用来生成一个生成器函数，每次调用next()函数，就执行一次yield\n",
    "            yield data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载分词器\n",
    "\n",
    "中文分词器库：Jieba、HanLP、THULAC\n",
    "英文及多语言分词器库：NLTK、SpaCy\n",
    "\n",
    "本项目使用Hugging Face的 `tokenizers` 库，侧重于为各类预训练模型服务，通用性更强，支持多种语言和多种分词算法（如 BPE、WordPiece 等），提供对分词模型的训练、词汇表管理等功能。\n",
    "\n",
    "`tokenizers` 库结构和组件：\n",
    "* Tokenizer：核心类，负责将文本转换为token\n",
    "* Normalizer：文本预处理，如转换为小写等\n",
    "* PreTokenizer：处理文本的初步分割\n",
    "* Model：定义如何将文本切割成token，支持BPE、WordPiece等\n",
    "* PostProcessor：处理tokens的最后步骤，如添加特殊标记\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "\n",
    "# 定义分词器\n",
    "tokenizer = Tokenizer(models.BPE())  # 使用BPE分词模型\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)  # 使用字节级别的预分词器，不在文本开头添加空格\n",
    "\n",
    "# 定义特殊Token\n",
    "special_tokens = [\"<unk>\", \"<s>\", \"/s\"]\n",
    "\n",
    "# 设置训练器\n",
    "trainer = trainers.BpeTrainer(vocab_size=6400,    \n",
    "    special_tokens=special_tokens,  # 确保这三个token被包含\n",
    "    show_progress=True,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文本数据\n",
    "texts = read_text_from_jsonl(data_path)\n",
    "# 训练 tokenizer\n",
    "tokenizer.train_from_iterator(texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存分类器\n",
    "设置解码器也为字节形式，保持一致。保存配置文件是为了方便复用和理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置解码器\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# 保存tokenizer\n",
    "tokenizer_dir = \"/root/autodl-tmp/miniDeepSeek/model/miniDeepSeek_tokenizer\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\"))\n",
    "tokenizer.model.save(\"/root/autodl-tmp/miniDeepSeek/model/miniDeepSeek_tokenizer\")\n",
    "\n",
    "# 手动创建配置文件\n",
    "config = {\n",
    "    \"add_bos_token\": False,\n",
    "    \"add_eos_token\": False,\n",
    "    \"add_prefix_space\": True,\n",
    "    \"added_tokens_decoder\": {\n",
    "        \"0\": {\n",
    "            \"content\": \"<unk>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "            },\n",
    "        \"1\": {\n",
    "            \"content\": \"<s>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "            },\n",
    "        \"2\": {\n",
    "            \"content\": \"</s>\",\n",
    "            \"lstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"single_word\": False,\n",
    "            \"special\": True\n",
    "            }\n",
    "    },\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"clean_up_tokenization_spaces\": False,\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"legacy\": True,\n",
    "    \"model_max_length\": 1000000000000000019884624838656,\n",
    "    \"pad_token\": None,\n",
    "    \"sp_model_kwargs\": {},\n",
    "    \"spaces_between_special_tokens\": False,\n",
    "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"use_default_system_prompt\": False,\n",
    "    \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<s>user\\\\n' + content + '</s>\\\\n<s>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '</s>' + '\\\\n' }}{% endif %}{% endfor %}\"\n",
    "}\n",
    "\n",
    "# 保存配置文件\n",
    "with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "    json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tokenizer 保存成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 加载预训练的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/miniDeepSeek_tokenizer\")\n",
    "\n",
    "# 测试一段对话\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个优秀的聊天机器人，总是给我正确的回应！\"},\n",
    "    {\"role\": \"user\", \"content\": '是椭圆形的'},\n",
    "    {\"role\": \"assistant\", \"content\": '456'},\n",
    "    {\"role\": \"user\", \"content\": '456'},\n",
    "    {\"role\": \"assistant\", \"content\": '789'}\n",
    "]\n",
    "\n",
    "# 使用模板进行文本处理\n",
    "new_prompt = tokenizer.apply_chat_template(messages, tokenize=True)\n",
    "print(new_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
