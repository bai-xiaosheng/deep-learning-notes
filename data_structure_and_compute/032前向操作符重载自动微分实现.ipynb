{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前向自动微分\n",
    "前向自动微分是指从计算图的起点开始，沿着计算图边的方向依次向前计算，最终达到计算图的终点。根据自变量的值计算出计算图中每个节点的值及其导数值，并保留中间结果，一直得到整个函数的值及其导数值。\n",
    "\n",
    "步骤：\n",
    "1. 将程序分解为一系列已知微分规则的基础表达式组合\n",
    "2. 根据已知微分规则给出各基本表达式的微分结果。  操作符重载的方法是指在负载运算过程中，根据已知微分规则给出各基础表达式的微分结果\n",
    "3. 根据链式法则计算出最终微分结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 具体实现\n",
    "以计算$f(x1, x2) = ln(x1) + x1 \\cdot x2 - sin(x2)$ 为例\n",
    "1. 首先导入 `Numpy` 库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 创建 `ADTangent` 类，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 操作符重写基本运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADTangent:\n",
    "    # 自变量 x，对自变量进行求导得到的 dx\n",
    "    def __init__(self, x, dx):\n",
    "        self.x = x\n",
    "        self.dx = dx\n",
    "    \n",
    "    # 重载 str 是为了方便打印的时候，看到输入的值和求导后的值\n",
    "    def __str__(self):\n",
    "        context = f\"value is {self.x:.4f}, grad is {self.dx}\"\n",
    "        return context\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x + other.x\n",
    "            dx = self.dx + other.dx\n",
    "        elif isinstance(other, float):\n",
    "            x = self.x + other\n",
    "            dx = self.dx\n",
    "        else:\n",
    "            raise ValueError(\"input must be ADTengent or float!\")\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x - other.x\n",
    "            dx = self.dx - other.dx\n",
    "        elif isinstance(other, float):\n",
    "            x = self.x - other\n",
    "            dx = self.dx\n",
    "        else:\n",
    "            return NotImplementedError\n",
    "        return ADTangent(x, dx)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x * other.x\n",
    "            dx = self.x * other.dx + self.dx * other.x\n",
    "        elif isinstance(other, float):\n",
    "            x = self.x * other\n",
    "            dx = self.dx * other\n",
    "        else:\n",
    "            return NotImplementedError\n",
    "        return ADTangent(x, dx)\n",
    "\n",
    "    def log(self):\n",
    "        x = np.log(self.x)\n",
    "        dx = 1 / self.x * self.dx\n",
    "        return ADTangent(x, dx)\n",
    "\n",
    "    def sin(self):\n",
    "        x = np.sin(self.x)\n",
    "        dx = self.dx * np.cos(self.x)\n",
    "        return ADTangent(x, dx)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 计算当 $x1=2,x2=5$ 时 $f$ 关于自变量 $x1$ 的导数，因此在数据初始化时，将自变量 $x1$ 的 $dx$ 设置为1，$x2$ 的 $dx$ 设置为0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value is 11.6521, grad is 5.5\n"
     ]
    }
   ],
   "source": [
    "x1 = ADTangent(x=2.0, dx=1)\n",
    "x2 = ADTangent(x=5.0, dx=0)\n",
    "f = ADTangent.log(x1) + x1 * x2 - ADTangent.sin(x2)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 实现结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.6521], grad_fn=<SubBackward0>)\n",
      "tensor([5.5000])\n",
      "tensor([1.7163])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.Tensor([2.]), requires_grad=True)\n",
    "y = Variable(torch.Tensor([5.]), requires_grad=True)\n",
    "f = torch.log(x) + x * y - torch.sin(y)\n",
    "f.backward()\n",
    "\n",
    "print(f)\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn_note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
