# 模型训练
## 训练过程

## 训练显存占用

# LLM推理
## 参考资料
[deepseek技术解读(1)-彻底理解MLA（Multi-Head Latent Attention](https://zhuanlan.zhihu.com/p/16730036197 "知乎 姜富春​ 中国科学院大学 计算机技术硕士")
## 推理过程
LLM推理过程主要包括两个阶段：Prefill 阶段和 Decode 阶段
* Prefill 阶段：一次性处理所有的prompt token，缓存所有输入对应的kv值，用于后面 Decode 阶段计算。计算密集型任务，计算复杂度为O（n^2）
* Decode 阶段：首先根据之前prefill阶段生成kv缓存和初始的上下文（通常是一个开始token,例如 $<\text{BOS}>$）生成第一个输出token，之后计算这个token对应的kv值，加入到缓存中，计算下一个token时会使用输入对应kv值和之前输出对应的kv值，直到最终输出EOS（end-of-sequence）token。只会看到当前词和当前词以前的输入。内存密集型任务，计算复杂度为O(n)
## 推理时显存占用
### 哪些数据会放到显存中
* 输入数据
* 模型参数量：推理时，这部分固定值
* KV缓存值：这部分在推理时占用会逐渐增加
* 中间计算值：
### 数据所占内存计算
假设模型包含 $l = 80$ 层，$n_h = 64$ 个头，每个头的向量维度为$d_h = 128$ （Qwen-72B参数配置）
那么一共需要 $n_{kv} = 2 * 80 * 64 = 10240$ 个 k 和 v。
参数量：$n_p = n_{kv} * d_h = 10240 * 128 = 1310720$
假设使用fp16精度(占两个字节)进行计算，那么所占内存为 $mem_{kv} = n_p * 2 = 2.5 M$

### 减少KV缓存占用的方法
1. 共享kv：将多个头的kv使用一个值，MQA：每一层所有的head共享一个kv，GQA：将head分组，每一组共享一个kv
2. 窗口kv：针对长序列，只保存窗口范围内的kv缓存，Longformer
3. 量化压缩：使用更低的bit保存缓存结果
4. 计算优化：减少访存换入换出的次数，如flashAttention